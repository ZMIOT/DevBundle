\section{Experiment and Discussion}
\label{sec:results}
In this section, we will show a series of experimental results including evaluation for co-segmentation and joint registration on synthetic data for quantitative analysis, investigation on the robustness of our method on point completeness and amount of user interaction, and testing on one group of real data.

\noindent \textbf{Synthetic Data Collection.} 
We generate a group of synthetic datasets ( synthetic point sets) for quantitatively evaluate our algorithm. 
%
For each dataset, we model a 3D scene using object models from 3D Warehouse.
We convert the mesh model of the scene into a point set using the Poisson sampling method~\cite{PossionSampling}.
%
Then we manually move the objects according to their functions and generate multiple point sets. 

\comments{To collect data of real scenes, we scan a scene at different times using \cite{VXH}.
%
The reconstructed mesh model of the scene is converted to point sets using Poisson sampling method~\cite{PossionSampling}.
% 
Since our approach focuses on objects in the scene, the wall and floor are removed by detecting large planes.
}
\subsection{Co-segmentation on Synthetic Data}
\begin{table}
	\centering
	\caption{Mean and standard deviation of IOU scores on two synthetic datasets. JRCS-Basic is our basic formulation. JRCS-Bilateral  is our bilateral formulation with point color as feature.}
	\begin{tabular}{c c c c}
		Datasets &  Study Room & Office Desk \\
		\hline
		JRCS-Basic & 0.808$\pm$0.032 & 0.831 $\pm$ 0.027\\   
		JRCS-Bilateral & 0.876$\pm$0.012 & 0.829 $\pm$ 0.028\\
		PointNet\cite{qi2016pointnet} & 0.402$\pm$0.032 &  0.439 $\pm$ 0.049\\
	\end{tabular}
	\label{tab:seg}
\end{table}
\begin{figure}[htb]
	\centering
	\includegraphics[width=\linewidth]{images/exp/exp_seg}
	\caption{\label{fig:seg} Segmentation evaluations on two groups of synthetic data (study room and office desk). Three examples of point set from each group are shown.}
\end{figure}
%
From the perspective of co-segmentation, we quantitatively evaluate our algorithm on two groups of synthetic data of indoor scenes. 
%
To estimate the power of the proposed algorithm, the interaction of placing boxes is only performed at one point set. No further interaction is required. 
% we only input layout for one point set in each group for initialization and do not add further interaction.
%
For numerical estimation, we calculate the intersection over union (IOU) scores for the inducing segmentation against the ground-truth segmentation.
% 
We compare our results with the state-of-the-art semantic segmentation method, PointNet~\cite{qi2016pointnet}, which trains a network using a large-scale database. 
%
Table~\ref{tab:seg} shows the numeric result and Figure~\ref{fig:seg} shows visual result of three input point sets including the one that is equipped with input layout.
For the object class that is not annotated in the training data, PointNet~\cite{qi2016pointnet} treats it as a special class of "clutter". This is why we have different ground truth for our method and PointNet. As shown in Figure~\ref{fig:seg}, we have "GT" as ground truth used to evaluate our method and "GT for PointNet" as ground truth used to evaluate PointNet. 
%
Comparing our method to PointNet is not an exact fair comparison in following aspects:
\begin{enumerate}
\item Our method allows user interaction while PointNet is fully automatic in the test phase.
\item Our synthetic data is quite different from the data in Stanford 3D semantic parsing dataset\cite{semsegdataset} which is used to train the semantic segmentation network of PointNet.
\item Our method outputs object-level segmentation without semantic label, while PointNet outputs semantic labels.  
\end{enumerate}
However, by comparison we can see that the generalization ability of current learning-based method is still far from enough to be used as tool to prepare data and build dataset. Semantic segmentation method is limited to certain set of object classes (13 classes for PointNet) and cannot be used to carry on our task. 
\subsection{Joint Registration on Synthetic Data}
From the perspective of joint registration, we first evaluate the result by transferring the point cloud of objects to each input point set based on result $\{\phi_{mn}\}$ and calculating the average distance from a point to its true correspondent point for each input point set.
We use this average distance as fitness error to evaluate the registration quality respect to each input set.

\begin{table}
	\centering
	\caption{Registration errors of the three groups of synthetic data in Figure~\ref{fig:seg}. The errors are measured in meter.}
	\begin{tabular}{c | c c c}
		Method@Dataset&Maximum&Median&Minimum\\
		\hline 
		Basic@Study Room&0.441&0.085&0.027\\
		Bilateral@Study Room&0.139&0.052&1.31e-05\\
	    Basic@Office Desk&0.309&0.0408&5.82e-03\\
		Bilateral@Office Desk&0.222&0.0574&8.33e-03\\
	\end{tabular}
	\label{tab:regerror}
\end{table}

Table~\ref{tab:regerror} shows the result of this evaluation. The Maximum, Median and Minimum of the fitness error across input sets are reported.
%
%For this evaluation we want to discuss that:\\
We find that even the input set with high IOU scores in segmentation can result in high fitness error. We believe this is due to the symmetric and near-symmetric objects in the scene. For symmetric objects, even the registration is correct the distance from a point to its true correspondent point can be high, since the rotation in registration result can be different from the one we use to generate this synthetic data. For near-symmetric objects, the registration often gets stuck in a local optimal and results in a high IOU score but a high fitness error. In Figure~\ref{fig:reg_colorcode}. we can see that the registration of the round carpet is correct but due to its symmetry its point-wise correspondences are not the same with identity transformation.
While the shelf corner highlighted in the red rectangles is not correctly aligned and it stucks at a local minimum that maps left part to right part.
\begin{figure}[htb]
	\centering
	\includegraphics[width=\linewidth]{images/exp/exp_reg}
	\caption{Joint registration results on two scenes using two variants of our method. Point-wise correspondences are color-coded.}
	\label{fig:reg_colorcode}
\end{figure}
We then compare our method (JRCS-Basic) with \cite{Evangelidis2014}(JRMPC) on the synthetic point sets released by \cite{Evangelidis2014}. These data contains four point sets of Stanford Bunny with different noise and outliers. From the experiment result shown in Table~\ref{tab:reg} and Figure~\ref{fig:reg}, we can see that when dealing with one object, our method have similar result with \cite{Evangelidis2014}.

\begin{table}
	\centering
	\caption{RMSE of joint registration on 4 point sets of Stanford Bunny by two methods.}
	\begin{tabular}{c c c c}
		Point Sets& View 2 & View 3 & View 4 \\
		\hline
		JRMPC & 0.1604 & 0.1719 & 0.1838\\   
		JRCS-Basic & 0.0822 &  0.1570  & 0.2301\\
	\end{tabular}
	\label{tab:reg}
\end{table}
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.4\linewidth]{images/exp/JRMPC.png}
	\includegraphics[width=0.4\linewidth]{images/exp/JRCSReg.png}
	\caption{Joint registration results on 4 point sets of Stanford Bunny by JRMPC~\cite{Evangelidis2014} (left) and our JRCS-Basic (right).}
	\label{fig:reg}
\end{figure}

\subsection{Amount of Interaction}
\label{subsec:interact}

For parameter initialization and object shape constraint, we only need the user to input layout (boxes) in one of the input point sets. However, our algorithm sometimes gets stuck at local minimum on handling non-local motion of objects. In such challenging cases, we require more user input to further guide the optimization. Figure~\ref{fig:interact_number} shows how the IOU score increases along with the amount of interaction. In this experiment, we use JRCS-Basic. In Figure~\ref{fig:interact_number}, the curve of Minimum IOU is not monotonically increasing with the amount of manual input, which means more interaction does not guarantee improvement of the segmentation results in all point sets.  When the initial correspondences in most point sets are far from correct our method loses its ability to transfer the information among different point sets. The further interaction only improves the segmentation in the point set which the user adds layout into and barely improves the segmentation in other point sets.
From Figure~\ref{fig:interact_vis}, we can see that actually quite a lot more interaction is needed for the overall segmentation result to be visually satisfying for the dataset in this experiment.
 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/interact/IOU.png}
	\caption{IOU scores of co-segmentation results based on different amount of user interaction. The $X$ axis is the ratio: $x=\frac{Input~Box~Number}{Total~Object~Number}$. $x=1.0$ means that the user places one box for each object in all point sets.}
	\label{fig:interact_number}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/interact/interact}
	\caption{Given the same input point sets, more accurate segmentation results can be obtained with more interaction. From left to right: 3 out of 16 input point sets, the ground-truth segmentation, our result when only one point set is equipped with manual input layout, and our result when 11 out of 16 point sets are equipped with manual input layout.
	}
	\label{fig:interact_vis}
\end{figure}

\subsection{Influence of Point Incompleteness}
\label{sec:exp-incompleteness}
In previous evaluation on synthetic data, we use data that the objects are completely covered by the sampled points. 
%
In this subsection, we investigate how the point set incompleteness affect the result of our algorithm. 
%To do this, we pick a group of point sets that can converge well ( IOU $> 99\%$ for each point set ) when the point sets are complete. 
%
To test this, we pick a group of point sets, and gradually remove certain percentage ($0\%-30\%$) of points from each point set. In order to simulate the point incompleteness caused by occlusion using a simple method, we generate the incomplete point sets with incompleteness of $p\%$ as follows:
\begin{enumerate}
	\item We randomly pick one point from each complete point set. 
	\item For one point set, sort all points ascending according to their euclidean distance to the picked point.  
	\item Remove the first $p\%$ points from the point set to generate a point set with incompleteness of $p\%$.
\end{enumerate}
% 
Figure~\ref{fig:incompleteness} shows how the IOU score is affected with the increasing point set incompleteness. 
The results under $p=\{0.0,14.0,30.0\}$ are shown. In Figure~\ref{fig:incompleteness2}(A09-E09), we can see that for some object in the scene half of the points are already removed. Even with serious incompleteness on some objects, our algorithm converges to a relative good result.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/incompleteness/IOU.png}
	\caption{IOU scores of co-segmentation with different data incompleteness. The used data is partially shown in Figure~\ref{fig:incompleteness2}.}
	\label{fig:incompleteness}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/incompleteness/visual}
	\caption{Experiments on data incompleteness. This figure shows results at 3 different level of incompleteness which are $0.0\%$ at row 01-04, $14\%$ at row 05-08 and $30\%$ at row 09-12. Each column shows the information of the same point set. Rows 01, 05, 09 show the inputs. 
		Column A shows one point set and the manual input for initialization. 
		The initial segmentation and final segmentation of this point set are shown in column A as well.
		Row 02,06,10 are ground-truth of segmentation. Row 03,07,11 are our segmentation results. 
		Row 04,08,12 shows the point-wise correspondences of joint registration by color-coding.}
	\label{fig:incompleteness2}
\end{figure}
\subsection{Test On Real Data}
To capture real data we employ the voxel hashing method~\cite{VXH} and use plane fitting to remove walls and floors. 
%
We then transfer the meshes into point sets using a Poisson sampling process~\cite{PossionSampling}.
%
Figure~\ref{fig:challenge} shows a scanned point set. We can see that, there are noised and blurred color, shape distortion, partial scanning and outliers in real data.
%
Figure~\ref{fig:realdata} shows the segmentation and registration results on a group of scanned point sets We uses JRCS-Bilateral in this test and Figure~\ref{fig:realdata}(d) shows the only point set that is equipped with layout in this test.
From Figure~\ref{fig:realdata}(e), we can see that all input point sets are partitioned into objects. In Figure~\ref{fig:realdata}(g), we align the point sets all together respecting to each of the objects. There are four objects in the scene, so there are four different aligned results in Figure~\ref{fig:realdata}(g). The light blue rectangle highlights the object that is used to align the point sets. We can verify that the objects from each input set are aligned together by the result transformation.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/challenge/challenge}
	\caption{\label{fig:challenge}Common challenges in scanned data.}
\end{figure}
\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{images/realdata/realdata}
	\caption{\label{fig:realdata} Segmentation and registration on real data. (a) Scanned mesh using method in \cite{VXH}. (b) Remove walls and floors by plane fitting. (c) Sampled point set using \cite{PossionSampling}. (d) With roughly placed boxes on only one point set, the points are initially segmented in this one point set. Note that parts of the chair legs are segmented to the table due to the rough box placement by users. (e) Pairs of input point sets and corresponding segmentation results. (f) The final Gaussian centroids for the five objects in the scene. (g) Verification of the registration result by aligning all point sets with respect to each object. The light blue rectangle highlights the object that is aligned together. Except the aligned object, the other objects are placed quite messy since they came from different point sets and have different arrangement relative to the aligned object. %\cxj{Again, i think (g) is really messy and confusing...}  
	}
\end{figure*} 
\section{Conclusion}
For the challenging problem of point set joint registration and co-segmentation, we come up with a formulation simultaneously modeling the two entangled sub-problems. For the difficult initialization and optimization of this formulation, we provide a strategy that leans on a few manual inputs. In the evaluation, our algorithm shows some success on both synthetic and real data.
The practical issue holding us back is the time performance of our current implementation, which prevents us from going over more initialization and optimization strategies. For a group of 11 point sets with about 9K points in each point set, our current implementation will take about 110 minutes to run 100 iterations.  
With a parallelized implementation, we can probably explore more potentials of our algorithm.
For example, we can try starting with object number of two and equal split of the Gaussian models. Then we can increase the object number by one and adjust the number of Gaussian models for each object based on the previous result and restart the optimization. Perhaps, we can get rid off the manual initialization with such procedure. To try it on a scene of larger scale we can use hierarchical representation and drawing experience from \cite{GOGMA} and \cite{AGM}. 