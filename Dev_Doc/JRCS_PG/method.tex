\section{Method Overview}
\label{sec:method}

\mdf{In this section, we introduce our formulation of the joint registration and co-segmentation problem for point sets. Tabel~\ref{tab:symbol} lists all the symbols used in our formulation. We well explain them in the following subsections.}


\subsection{Problem Statement}
Given $M$ point sets $\mathcal{V}=\{\mathbf{V}_m\}^{M}_{m=1}$ where $\mathbf{V}_m=\{\mathbf{v}_{mi}\}^{L_m}_{i=1}$ contains $L_m$ 3D points, our problem is to simu\mdf{l}taneously partition the point sets into $N$ objects and  find the correspondences from objects to the point sets.
We represent each object $\vb{O}_{n}$ as a set of 3D points $\vb{O}_{n}=\{\vb{x}_{nk}\}^{K_n}_{k=1}$. 
%
Each object model $\vb{O}_{n}$ is rigidly transformed to each point set $\vb{V}_m$ with a transformation matrix $\phi_{mn}(\vb{x}_{nk})=\vb{R}_{mn}\vb{x}_{nk}+\vb{t}_{mn}$.
%
\mdf{For robustness, we do not model the point sets as a simple composition of transformed 3D points in each object model. Instead, we model the point sets as realizations of an unknown central Gaussian mixture model (GMM) from of the transformed object models.
}
%
%The points that belong to the same object are rigidly transformed realization of a GMM, whose centroids are the 3D points of the object model. 
Hence, for each point $\vb{v}_{mi}$ in the point set $\vb{V}_m$, given the object model $\vb{O}_{n}$ and its rigid transformation to the point set $\vb{V}_m$, we can write
\begin{equation}
\label{equ:model}
P(\vb{v}_{mi})=\sum^{K_n}_{k=1}p_{k}\mathcal{N}(\vb{v}_{mi}|\phi_{mn}(\vb{x}_{k}),\Sigma_{k}).
\end{equation}
%
 
%
% rigid transformations $\{\phi_{mn}(\vb{x})=\vb{R}_{mn}\vb{x}+\vb{t}_{mn}\}$ that transform each object model to its observed position in each input point set.  
%$V_m=\{\pmb{v}_{mi}\}$ for the $m^{th}$ point set (Figure~\ref{fig:teaser} (a) shows an example of input point sets.), we intend to simu\mdf{l}taneously partition the point sets into $N$ objects and find the rigid transformations $\{\phi_{mn}(\pmb{x})=R_{mn}\pmb{x}+\pmb{t}_{mn}\}$ that transform each object model to its observed position in each input point set.  

For partitioning, we output point-wise label vectors $\{\vb{y}_m\}$ for each input point set to indicate its object partition. Figure~\ref{fig:teaser}~(b) illustrates the label vectors by assigning same color to points with the same label. 
%
%To establish our formulation, we first summarize the symbols that will be used in Table \ref{tab:symbol}.
\begin{table}[!hbp]
\centering
\caption{Table of symbols used in the paper. \cxj{Put the caption of a table on the top, while figure captions are below figures.}} 
\label{tab:symbol}
\begin{tabular}{c|l}
\hline
Symbol         & Definition\\
\hline
$M$            & The number of input point sets.\\
$\mathbf{V}_m$ & The $m^{th}$ input point set.\\
$\mathcal{V}$   & The input point sets $\{\mathbf{V}_m\}^{M}_{m=1}$.\\
$N_m$          & The number of points \mdf{of the} $m^{th}$ point set $\mathbf{V}_m$.\\
$\vb{v}_{mi}$  & The $i^{th}$ point of $\vb{V}_m$.\\
$\vb f_{mi}$   & The point-wise feature vector of $\vb v_{mi}$.\\
$z_{mi}$       & The latent parameter for $\vb v_{mi}$.\\
               & $z_{mi}=k$ means $\vb{v}_{mi}$ is generated by $k^{th}$ Gaussian. \\
               &\cxj{no object info?}\\
$Z$            & $Z=\{z_{mi}|m=1...M,i=1...L_m\}$.\\
$N$            & The number of objects \mdf{in the scene}.\\
$K_n$          & The number of Gaussian for $n^{th}$ object. \\
$K_{all}$      & The total number of Gaussian models \mdf{of all objects}. \\
               & $\sum_n^N K_n = K_{all}$.\\
$p_k$          & The weight of $k^{th}$ Gaussian. $\sum_k^{K_{all}}p_k=1$.\\
$\vb x_k$      & The centroid of $k^{th}$ Gaussian model.\\
$\vb {x}^{v}_k$   & The centroid of $k^{th}$ Gaussian for point position.\\
$\vb {x}^{f}_k$   & The centroid of $k^{th}$ Gaussian for point feature.\\
$\Sigma_k$     & The covariance matrix of $k^{th}$ Gaussian model.\\
$\sigma_k$     & $\Sigma_k=\sigma_k^2\vb{I}$, where $\vb{I}$ is an identity matrix.\\
$\sigma^v_k$   & Gaussian covariance parameter for point position\\
$\sigma^f_k$   & Gaussian covariance parameter for point feature\\
$\phi_{mn}$    & Rigid transformation from object $\vb{O}_n$ to point set $\vb{V}_m$.\\
%$\vb{R}_{mn}$ & The rotation matrix for $\phi_{mn}$.\\
%$\vb{t}_{mn}$ & The translation vector for $\phi_{mn}$.
\hline
\end{tabular}
\end{table}


\comments{
\subsection{Basic Formulation}


To simutaneously solve the joint registration and co-segmentation, we come up with a generative model as follows:
\begin{equation}
\label{equ:model}
P(\pmb{v}_{mi})=\sum^{K_n}_{k=1}p_k\mathcal{N}(\pmb{v}_{mi}|\phi_{mn}(\pmb{x}_k),\Sigma_k)
\end{equation}
which treat the $i^{th}$ observed point $v_{mi}$ from the $m^{th}$ point set as a sample point generated by one of $N$ object models.

Each object model is represented by a group of $K_n$ gaussian models.
}


\mdf{Given the generative representation of point sets, the unknown model parameters of our joint registration and segmentation problem are}
%
\begin{displaymath}
\varTheta=\{\{p_k,\vb{x}_k,\Sigma_k\}_{k=1}^{K_{all}},\{\phi_{mn}\}_{m=1,n=1}^{MN}, \vb{y}_{m}\}.
\end{displaymath}
\cxj{Are object labels of points also unknown model parametes?}


The problem is how to estimate the model parameters $\Theta$ to fit all the input point sets. \mdf{Without knowing object labels for all 3D points,} the problem can be solved in the EM framework of expectation- maximization. 
%
In particular, we bring in a latent parameter as: \cxj{why do not use hidden variables instead of using latent parameters?}
\begin{equation}
\mathcal{Z}=\{z_{mi}|m=1...M,n=1...L_m\},
\end{equation}
%
such that $z_{mi}=k(k=1,2...,K_{all})$ assigns the observed point $v_{mi}$ to the $k^{th}$ component of the Gaussian mixture model. 
%
We aim to maximize the expected complete-data log-likelihood:
\begin{equation}
\label{equ:obj0}
\mathcal{E}(\Theta|\mathcal{V},\mathcal{Z})=\mathbb{E}_{\mathcal{Z}}[\ln P(\mathcal{V},\mathcal{Z};\Theta)|\defV]={\sum_{\mathcal{Z}}P(\defZ|\defV,\Theta)\ln{P(\mathcal{V},\mathcal{Z};\Theta)}}.
\end{equation}


This formulation can be seen as an adaption of \mdf{the} joint registration formulation in \cite{Evangelidis2014}, upon which we separate Gaussian models into groups to express multiple objects. 
%
The latent parameter $\defZ$ that assigns observed points to Gaussian models can naturally indicate the object level segmentation.
%
Under the asssumption \cxj{->assumption} that the input points are independent and identically distributed, we can rewrite the objective defined in Eq.~(\ref{equ:obj0}) into:
%
\begin{equation} \label{equ:obj2}
\Theta=\arg\max\sum_{mik}\alpha_{mik}(\ln p_k + \ln P(\vb{v}_{mi}|z_{mi}=k;\Theta)),
\end{equation}
%
where $\alpha_{mik} = P( z_{mi} = k | \vb{v}_{mi} ; \Theta )$.


By bringing in Eq.~\ref{equ:model} and ingnoring \cxj{->ignoring} constant terms, we can rewrite the objective as:
\begin{equation}
\label{equ:obj3}
\Theta=\arg\max\sum_{mik}\alpha_{mik}(||\vb{v}_{mi}-\phi_{mn}(\vb{x}_k)||_{\Sigma_k}^2 + \ln |\Sigma_k| - 2\ln p_k), 
\end{equation}
%
where the $|\cdot|$ denotes the determinant and $||\vb{x}||_\vb{A}^2=\vb{x}^T\vb{A}^{-1}\vb{x}$. 
%
It is predefined that $\vb{x}_k$ is one of the \mdf{G}aussian centroid\mdf{s} used to represent $n^{th}$ object, which is why we apply transformation $\phi_{mn}$ on to the $\vb{x}_k$. 
%
For the convenience of computation, we restrict the model to isotropic covariances, i.e.,$\Sigma_k=\sigma^2\vb{I}$ and $\vb{I}$ is the identity matrix.



Now, we can optimize this through iterating between estimating $\alpha_{mik}$ (Expectation-step) and maximizing $f(\Theta|\defV,\defZ)$ sequentially with respect to each parameters in $\Theta$ (Maximization-steps).
%
%These steps are:

\textbf{E-step}:
this step estimates the posterior probability $\alpha_{mik}$ of $v_{mi}$ to be a point generated by the $k^{th}$ Gaussian model.
%
\begin{equation}
\label{equ:estep}
\alpha_{mik}=\frac{p_k\sigma_k^{-3}exp(-\frac{1}{2\sigma_k^2}||\pmb v_{mi}-\phi_{mn}(\pmb x_k)||^2)}{\sum_s^{K_{all}}p_s\sigma_s^{-3}exp(-\frac{1}{2\sigma_s^2}||\pmb v_{mi}-\phi_{mn}(\pmb x_s)||^2)}
\end{equation}
%


\textbf{M-step-a}: this step update\mdf{s} the transformations $\phi_{mn}$ that maximize $f(\Theta)$, given instant values for $\alpha_{mik}$, $\vb{x}_k$, $\sigma_k$. 
\cxj{What is $f(\Theta)$?}
%
We only consider rigid transformations, making  $\phi_{mn}(x)=\vb{R}_{mn}\vb{x}+\vb{t}_{mn}$. The maximizer $\vb{R}_{mn}^*,\vb{t}_{mn}^*$ of $f(\Theta)$ is the same with the minimizers of the following constrained optimization problems
%
\begin{equation}
\left\{
\begin{array}{rcl}
\min_{R_{mn},t_{mn}}&      &||(W_{mn}-R_{mn}X_n-\pmb t_{mn}\mathbf{e}^T)\Lambda_{mn}||_F^2\\
s.t.&      &R_{mn}^TR_{mn}=I, |R_{mn}|=1\\
\end{array} \right.
\end{equation}
where $\Lambda_{mn}$ is $K_n \times K_n$ diagonal matrix with elements $\lambda_{mnk}=\frac{1}{\sigma_k}\sqrt{\sum_i^{I_{m}}\alpha_{mik}}$,$I_m$ is the number of point for the $m^{th}$ input point set, $X_n = [\vb{x}_1, \vb{x}_2,...., \vb{x}_{K_n}]$ is the matrix stacked by the centroids of gaussian models that are predefined to represent the $n^{th}$ object. $\mathbf{e}^T$ is a vector of ones, $||\cdot||_F$ denotes the Frobenius norm, and $W_{mn}=[w_{m1},w_{m2},...,w_{mk},...,w_{mK_n}]$, in which $w_{mk}$ is a weighted point as
%
\begin{equation}
w_{mk}=\frac{\sum_{i=1}^{L_m}\alpha_{mik} \vb{v}_{mi}}{\sum_{i=1}^{L_m}\alpha_{mik}}
\end{equation}




This problem has a similar solution with \cite{Evangelidis2014}. 
The only difference is that we are estimating the transformation from Gaussian models to the input point sets instead of the transformation from input point sets to Gaussian models, since there are multiple group of $\vb{x}_k$ corresponding to multiple objects in our Gaussian models. The optimal can be given by:
%
\begin{equation}
\label{equ:updateR}
\vb{R}_{mn}^*=\vb{U}_{mn}\vb{C}_{mn}\vb{V}_{mn}^T
\end{equation}
\begin{equation}
\label{equ:updatet}
\vb{t}_{mn}^*=\frac{1}{tr(\Lambda_{mn}^2)}(W_{mn}-\vb{R}_{mn}X_n)\Lambda_{mn}^2\mathbf{e}
\end{equation}
where $[\vb{U}_{mn},\vb{S},\vb{V}_{mn}]=svd( \vb{W}_{mn}\Lambda_{mn}P_{mn}\Lambda_{mn}X_{mn}^T )$ and $P_{mn}=I-\frac{\Lambda_{mn}\mathbf{e}(\Lambda_{mn}\mathbf{e})^T}{(\Lambda_{mn}\mathbf{e})^T\Lambda_{mn}\mathbf{e}}$,$I$ is identity matrix. $C_{mn}=diag(1,1,|U_{mn}||V_{mn}|)$.
\cxj{Change all matrixes such as R, t, U, v, x and so on from italian to bold font. use 'mathbf'. }

\textbf{M-step-b}: this step we update the parameters related to the Gaussian mixture model and the indicating vector for object segmentation 
\begin{equation}
\label{equ:updatexk}
\pmb x_k^*=\frac{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}(R_{mn}^{-1}v_{mi}-\pmb t_{mn})}{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}}
\end{equation}
where $\vb{x}_k$ is one of the Gaussian centroids that is predefined to represent \mdf{the} $n^{th}$ object. 
\begin{equation}
\label{equ:updatesigma}
\sigma_k^{*2}=\frac{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}||(v_{mi}-\pmb t_{mn}-R_{mn}^*\pmb x_k^*)||_2^2}{3\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}}
\end{equation}
\begin{equation}
\label{equ:updatepk}
p_k^*=\frac{\sum_{m,i}\alpha_{mik}}{M}
\end{equation}
\begin{equation}
\label{equ:updatey}
y_{m}(i)^*=\arg \max_n \sum_{k=K_{n-1}+1}^{K_n} \alpha_{mik} 
\end{equation}
where $y_{m}(i)$ is the $i^{th}$ entry of the indicate vector $y_m$.
\subsection{Bilateral Formulation}
When considering point-wise features, we can add bilateral terms into the generative model.
\begin{equation}
P(\vb{v}_{mi},f_{mi})=\sum^{K_n}_{k=1}p_k\mathcal{N}(v_{mi}|\phi_{mn}(\vb{xv}_k),\sigma v_k)\mathcal{N}(f_{mi}|\vb{xf}_k,\sigma f_k),
\end{equation}
where $f_{mi}$ is the feature vector for point $v_{mi}$ and $xf_k$ is the feature vector for $k^{th}$ point in latent model. As shown in the formulation, there is no transformation applyed onto $xf_k$, which means that this formulation is only suitable to the features that is rotation and translation invariant. For example, the point color vector(for all the result in this paper we use RGB color as feature vector ) $[red_{mi},green_{mi},blue_{mi}]$ is a suitable feature for this formulation. In this formulation $N(v_{mi}|\phi_{mn}(xv_k),\sigma v_k)$ is the spatial term and $N(f_{mi}|xf_k,\sigma f_k)$ is the feature term.
For the bilateral formulation, iteration steps will be as follows:\\
\textbf{E-step}:in this step the calculation of posterior probability need to consider both the spatial term and the feature term.
\begin{equation}
\label{equ:bestep}
\alpha_{mik}=\frac{p_kP_v( \vb{v}_{mi},\phi_{mn}(\vb{x}v_k),\sigma v_k)P_f(f_{mi},xf_k,\sigma f_k)}{\sum_s^{K_{all}}p_sP_v( \pmb v_{mi},\phi_{mn}(xv_s),\sigma v_s)P_f(f_{mi},xf_k,\sigma f_s)}
\end{equation}
where $P_v(\vb{x},y,\sigma)=\sigma^{-3}exp(-\frac{1}{2\sigma^2}||\vb{x}-y||^2)$ and $P_f(\vb{x},y,\sigma)=\sigma^{-D(\vb{x})}exp(-\frac{1}{2\sigma^2}||\vb{x}-y||^2)$ and $D(\vb{x})$ means the dimension of the vector $\pmb x$. 




\textbf{M-step-a:}for bilateral formulation, this step is the same with the basic formulation and the update can be done as (\ref{equ:updateR}) and (\ref{equ:updatet}).

\textbf{M-step-b}:for bilateral formulation, this step need not only update model centroids and variance for spatial term as (\ref{equ:updatexk}) and (\ref{equ:updatesigma}).
but also update the centroids and variance for feature term as in (\ref{equ:updatefk}) and (\ref{equ:updatefsigma}).

\begin{equation}
\label{equ:updatefk}
xf_k^*=\frac{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}f_{mi}}{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}}
\end{equation}
\begin{equation}
\label{equ:updatefsigma}
\sigma f_k^{*2}=\frac{\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}||(f_{mi}-xf_k^*)||_2^2}{D(f)\sum_{m=1}^M\sum_{i=1}^{I_m}\alpha_{mik}},
\end{equation}
where $D(f)$ is the dimenssion of feature vectors.



The update of $p_k$ for bialateral \cxj{bilateral?} formulation is the same as the basic formulation in Eg.~(\ref{equ:updatepk}).



\subsection{Interaction Design}
\label{sec:imp:interact}
Unfortunately, there are a large number of parameters that can not be easily initialized in our formulation. 
%
In this subsection we first introduce our design of interaction, which is intuitive for users to input the semantic prior this way. We then explain how we can easily initialize those parameters for our optimization based on the manual input.


As demonstrated in Figure~\ref{fig:interact}, we let user choose one of the point sets and place boxes in it to indicate the layout for this point set. From this, we can easily initialize the total number of objects $N$ and determine $\{K_n\}$ which is the numbers of Gaussian mixture models used to represent each object.
\begin{figure}[htb]
	\centering
	\includegraphics[width=.3\linewidth]{images/interact01.png}
	\includegraphics[width=.3\linewidth]{images/interact02.png}
	\includegraphics[width=.3\linewidth]{images/interact03.png}
	\includegraphics[width=.3\linewidth]{images/interact04.png}
	\includegraphics[width=.3\linewidth]{images/interact05.png}
	\includegraphics[width=.3\linewidth]{images/interact06.png}
	\caption{\label{fig:interact}
		From the first to the nineth, the nine images show the procedure of interaction: \cxj{there are only six images, not nine.}
		the user pick one point set and place boxes in it to indicate the layout for this point set. The box in white is the box currently under editing. The boxes in other colors are boxes placed to represent object layouts. One color represent\mdf{s} one object. The interaction allows multiple boxes to represent\mdf{s} same object.(e.g. the desk is represented by three boxes in same color)}
\end{figure}
These two paremeters \cxj{->parameters} are difficult to be initialized without semantic prior, but with the input of the users we can naturally initialize the $N$ as the number of different color label and the ${K_n}$ as 
\begin{equation}
\label{equ:K_n}
K_n=\frac{V_n}{\sum V_n}K_{all},
\end{equation}
in which the $V_n$ represent the total volume of the boxes in the $n^{th}$ color and the $K_{all}$ is initialized as $K_{all}=\frac{median(I_m)}{2}$ and $\{I_m\}$ are point numbers of $M$ input point sets. This is an emperical \cxj{->empirical} choice borrowed from \cite{Evangelidis2014}.\\
%
The expectation maximizaton \cxj{->maximization} framework is easily converged to a local optimal. To cope with this problem we further use this layout (boxes) from interaction as a soft constraint to guide the optimization and constrain the shape of generated object model. Such constraint is enforced by simply altering the posterior probability $\alpha_{mik}$ as
%
\begin{equation}
\label{equ:alteralpha}
\alpha_{mik}^*=\frac{\alpha_{mik}\beta_{mik}}{\sum_{i,k}\alpha_{mik}\beta_{mik}}
\end{equation}
where the $\beta_{mik}$ is the prior probability according to the boxes. It is defined as:
\begin{equation}
\beta_{mik}=\left\{
\begin{array}{rcl}
1& &\pmb v_{mi} \in B_n\\
\exp(-\frac{\min_{\pmb v_{mj}}|| \pmb v_{mi} - \pmb v_{mj} ||_2^2  )}{L})& &\pmb v_{mi} \notin B_n~and~\pmb v_{mj} \in B_n\\
\end{array} \right.
\end{equation}
where the $B_n$ is a point set that is enclosed by the boxes used to represent the layout of $n^{th}$ object. The $k^{th}$ Gaussian model is predefined to be one of the Gaussians used to represent $n^{th}$ object. $\min_{v_{mj}}|| v_{mi} - v_{mj} ||_2^2$ is actually the squared euclidean distance from point $v_{mi}$ to the point set $B_n$, as we define the distance from a point to a point set as the minimum distance from the point to any point inside the point set. $L$ here is a constant number with $L=2r^2$, and $r$ is the meadian \cxj{->median?} of the radius of input point sets. The radius of a point set is half of length of diagnonal \cxj{->diagonal?} line of its \mdf{axis-aligned} bounding box.   
\begin{figure}[htb]
	\centering
	\includegraphics[width=\linewidth]{images/localoptimal/localoptimal}
	\caption{\label{fig:localoptimal}This figure shows an example result when converges to a local optimal. (a) is the result of segmentation of this local optimal. (b) is the final centroids of latent model. It shows that from top to down the 2nd and 3rd object model both include part of the table and part of the chair.}
\end{figure}
This alteration on posterior probability is only done with the probability related to the $m^{th}$ point set that have the mannual input layout (the boxes) in it. This alteration can help prevent the optimization from converging to a local optimal as in Figure~\ref{fig:localoptimal}. The result from the Figure~\ref{fig:localoptimal} have \cxj{->has?} the same input and initialization with the result from Figure~\ref{fig:teaser}, but it doesn't \cxj{does not} use the posterior alteration as a soft constraint.

